{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc1214dc-246c-443d-936c-744e736f86e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset, load_metric\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de57c3b2-9ea0-478f-aa1a-181b9f0909f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"surrey-nlp/PLOD-CW\")\n",
    "# Vocab dictionary\n",
    "word_index = {}\n",
    "# Model parameters\n",
    "embedding_dim = 300\n",
    "hidden_dim = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fe1346d-3882-493e-bf3c-cbfd71c904cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset[\"train\"]\n",
    "val_dataset = dataset[\"validation\"]\n",
    "test_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50cb5420-2314-49e9-909c-0a43324abaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoding = {\"B-O\": 0, \"B-AC\": 1, \"B-LF\": 2, \"I-LF\": 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbb0b6f2-1009-4f89-917d-0d9abd7298d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_label(tags):\n",
    "    global label_encoding\n",
    "    return [label_encoding[tag] for tag in tags]\n",
    "\n",
    "def build_vocab(dataset):\n",
    "    global  word_index\n",
    "    for item in dataset['tokens']:\n",
    "        for word in item:\n",
    "            if word not in word_index:\n",
    "                word_index[word] = len(word_index)\n",
    "    return word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e81edece-a7a7-452a-b4a3-a60cafbcd92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = build_vocab(train_dataset)\n",
    "word_index = build_vocab(val_dataset)\n",
    "word_index = build_vocab(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd8761d9-943a-4434-b355-019f7758f538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LSTM model\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tags):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, tags)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        lstm_out = self.fc(lstm_out)\n",
    "        return torch.log_softmax(lstm_out, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc190a67-4f69-454e-91db-bc46ef43f854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode strings amd return tensor\n",
    "def encode_sequence(seq, encoder=None):\n",
    "    if type(seq[0]) == int:\n",
    "        encoded_ids = seq\n",
    "    else:\n",
    "        encoded_ids = [encoder[word] for word in seq]\n",
    "    return torch.tensor(encoded_ids, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56791da2-d617-4172-9708-fc369206fd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "def evaluate(model, token_dataset, word_index):\n",
    "    global label_encoding\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, datapoint in enumerate(token_dataset):\n",
    "            inputs = encode_sequence(datapoint['tokens'], word_index)\n",
    "            targets = encode_sequence(encode_label(datapoint['ner_tags']))\n",
    "            tag_scores = model(inputs.unsqueeze(0))\n",
    "            \n",
    "            predicted_tags = tag_scores.max(2)[1].squeeze().tolist()\n",
    "            y_pred.extend(predicted_tags)\n",
    "            y_true.extend(targets.tolist())    \n",
    "\n",
    "    # Detailed performance metrics\n",
    "    labels_indices = list(label_encoding.values())\n",
    "    labels_names = list(label_encoding.keys())\n",
    "    print(classification_report(y_true, y_pred, labels=labels_indices, target_names=labels_names))\n",
    "\n",
    "    overall_f1 = f1_score(y_true, y_pred, average= 'macro')\n",
    "    overall_precision = precision_score(y_true, y_pred, average='macro')\n",
    "    overall_recall = recall_score(y_true, y_pred, average='macro')\n",
    "    print(f'Overall F1 Score: {overall_f1}')\n",
    "    print(f'Overall Precision Score: {overall_precision}')\n",
    "    print(f'Overall Recall Score: {overall_recall}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dc7aebb3-97f3-49ca-9c87-05b11083b077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         B-O       0.90      0.95      0.93      4292\n",
      "        B-AC       0.63      0.35      0.45       270\n",
      "        B-LF       0.32      0.21      0.26       150\n",
      "        I-LF       0.52      0.41      0.46       288\n",
      "\n",
      "    accuracy                           0.87      5000\n",
      "   macro avg       0.59      0.48      0.52      5000\n",
      "weighted avg       0.85      0.87      0.85      5000\n",
      "\n",
      "Overall F1 Score: 0.5218138803785223\n",
      "Overall Precision Score: 0.5935364863400443\n",
      "Overall Recall Score: 0.4798173065306686\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "model = LSTM(embedding_dim, hidden_dim, len(word_index), 4)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.9)\n",
    "\n",
    "for epoch in range(6):\n",
    "    for i, datapoint in enumerate(train_dataset):\n",
    "        model.zero_grad()\n",
    "        sentence_in = encode_sequence(datapoint['tokens'], word_index)\n",
    "        targets = encode_sequence(encode_label(datapoint['ner_tags']))\n",
    "        tag_scores = model(sentence_in.unsqueeze(0))\n",
    "        loss = loss_function(tag_scores.squeeze(0), targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # if i == 199:break\n",
    "    \n",
    "    # Model Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for val_datapoint in val_dataset:\n",
    "            val_inputs = encode_sequence(val_datapoint['tokens'], word_index)\n",
    "            val_targets = encode_sequence(encode_label(val_datapoint['ner_tags']))\n",
    "            val_tag_scores = model(val_inputs.unsqueeze(0))\n",
    "            val_loss = loss_function(val_tag_scores.squeeze(0), val_targets)\n",
    "            \n",
    "    model.train()\n",
    "    \n",
    "# Evaluate on the test set\n",
    "evaluate(model, test_dataset, word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2072c7c6-a2a9-4bd5-8eb9-de7dc7cfac08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3685c241-5c20-4562-b6c9-1bcfc6ca121d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch.utils.data import DataLoader, Dataset\n",
    "# from datasets import load_dataset, load_metric\n",
    "# import numpy as np\n",
    "# from sklearn.metrics import f1_score, precision_score, recall_score, classification_report\n",
    "\n",
    "# dataset = load_dataset(\"surrey-nlp/PLOD-CW\")\n",
    "# # Model parameters\n",
    "# embedding_dim = 300\n",
    "# hidden_dim = 300\n",
    "\n",
    "# short_dataset = dataset[\"train\"][:200]\n",
    "# train_dataset = dataset[\"train\"]\n",
    "# val_dataset = dataset[\"validation\"]\n",
    "# test_dataset = dataset[\"test\"]\n",
    "\n",
    "# label_encoding = {\"B-O\": 0, \"B-AC\": 1, \"B-LF\": 2, \"I-LF\": 3}\n",
    "\n",
    "# label_list = []\n",
    "# for sample in short_dataset[\"ner_tags\"]:\n",
    "#     label_list.append([label_encoding[tag] for tag in sample])\n",
    "\n",
    "# val_label_list = []\n",
    "# for sample in val_dataset[\"ner_tags\"]:\n",
    "#     val_label_list.append([label_encoding[tag] for tag in sample])\n",
    "\n",
    "# test_label_list = []\n",
    "# for sample in test_dataset[\"ner_tags\"]:\n",
    "#     test_label_list.append([label_encoding[tag] for tag in sample])\n",
    "\n",
    "\n",
    "# # Step up dictionary\n",
    "# word_index = {}\n",
    "# def word_to_index(dataset):\n",
    "#     global word_index\n",
    "    \n",
    "#     for item in dataset['tokens']:\n",
    "#         for word in item:\n",
    "#             if word not in word_index:\n",
    "#                 word_index[word] = len(word_index) \n",
    "\n",
    "\n",
    "# word_to_index(train_dataset)\n",
    "# word_to_index(val_dataset)\n",
    "# word_to_index(test_dataset)\n",
    "\n",
    "\n",
    "# # Define the LSTM model\n",
    "# class LSTM(nn.Module):\n",
    "#     def __init__(self, embedding_dim, hidden_dim, vocab_size, tags):\n",
    "#         super(LSTM, self).__init__()\n",
    "#         self.hidden_dim = hidden_dim\n",
    "#         self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "#         self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "#         self.fc = nn.Linear(hidden_dim, tags)\n",
    "\n",
    "#     def forward(self, sentence):\n",
    "#         embeds = self.embeddings(sentence)\n",
    "#         lstm_out, _ = self.lstm(embeds)\n",
    "#         lstm_out = self.fc(lstm_out)\n",
    "#         return torch.log_softmax(lstm_out, dim=-1)\n",
    "\n",
    "\n",
    "# # Encode strings amd return tensor\n",
    "# def encode_sequence(seq, encoder=None):\n",
    "#     if type(seq[0]) == int:\n",
    "#         encoded_ids = seq\n",
    "#     else:\n",
    "#         encoded_ids = [encoder[word] for word in seq]\n",
    "#     return torch.tensor(encoded_ids, dtype=torch.long)\n",
    "\n",
    "\n",
    "# # Evaluate the model\n",
    "# def evaluate(model, test_data, test_tags, word_index):\n",
    "#     global label_encoding\n",
    "#     model.eval()\n",
    "#     y_true = []\n",
    "#     y_pred = []\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for i in range(len(test_data)):\n",
    "#             inputs = encode_sequence(test_data[i], word_index)\n",
    "#             targets = encode_sequence(test_tags[i])\n",
    "#             tag_scores = model(inputs.unsqueeze(0))\n",
    "\n",
    "#             predicted_tags = tag_scores.max(2)[1].squeeze().tolist()\n",
    "#             y_pred.extend(predicted_tags)\n",
    "#             y_true.extend(targets.tolist())    \n",
    "\n",
    "#     # Detailed performance metrics\n",
    "#     labels_indices = list(label_encoding.values())  # Numerical indices for the tags\n",
    "#     labels_names = list(label_encoding.keys())      # Corresponding names for the indices\n",
    "#     print(classification_report(y_true, y_pred, labels=labels_indices, target_names=labels_names))\n",
    "\n",
    "#     overall_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "#     overall_precision = precision_score(y_true, y_pred, average='macro')\n",
    "#     overall_recall = recall_score(y_true, y_pred, average='macro')\n",
    "#     print(f'Overall F1 Score: {overall_f1}')\n",
    "#     print(f'Overall Precision Score: {overall_precision}')\n",
    "#     print(f'Overall Recall Score: {overall_recall}')\n",
    "\n",
    "\n",
    "# # Training the model\n",
    "# model = LSTM(embedding_dim, hidden_dim, len(word_index), len(label_encoding))\n",
    "# loss_function = nn.NLLLoss()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# for epoch in range(10):\n",
    "#     for i in range(len(short_dataset['tokens'])):\n",
    "#         model.zero_grad()\n",
    "#         sentence_in = encode_sequence(short_dataset['tokens'][i], word_index)\n",
    "#         targets = encode_sequence(label_list[i])\n",
    "#         tag_scores = model(sentence_in.unsqueeze(0))\n",
    "#         loss = loss_function(tag_scores.squeeze(0), targets)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "# # Evaluate on the test set\n",
    "# evaluate(model, test_dataset['tokens'], test_label_list,word_index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
