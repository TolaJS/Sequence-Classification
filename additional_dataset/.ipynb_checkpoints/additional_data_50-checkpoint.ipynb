{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc1214dc-246c-443d-936c-744e736f86e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import spacy\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import seaborn as sns\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a9362fe-b6f1-49ce-9e5b-0670a6d87995",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de57c3b2-9ea0-478f-aa1a-181b9f0909f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset(\"surrey-nlp/PLOD-CW\")\n",
    "dataset2 = load_dataset(\"surrey-nlp/PLOD-filtered\")\n",
    "# Vocab dictionary\n",
    "word_index = {}\n",
    "# Model parameters\n",
    "embedding_dim = 300\n",
    "hidden_dim = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50cb5420-2314-49e9-909c-0a43324abaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoding = {\"B-O\": 0, \"B-AC\": 1, \"B-LF\": 2, \"I-LF\": 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1a134a2-e284-441d-a6fc-c6a630391fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_encoding = {0: \"B-O\", 1: \"B-AC\", 3: \"B-LF\", 4: \"I-LF\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ad873bf-f5ee-4f2f-8c62-378293398e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_dict(dataset, subset, size):\n",
    "    train2 = dataset2[subset][:1500]\n",
    "    temp_tokens, temp_pos, temp_ner = [], [], []\n",
    "    \n",
    "    for tokens, pos_tags, ner_tags in zip(train2['tokens'], train2['pos_tags'], train2['ner_tags']):\n",
    "        if tokens not in dataset['tokens']:\n",
    "            temp_tokens.append(tokens)\n",
    "            temp_pos.append(pos_tags)\n",
    "            temp_ner.append(ner_tags)\n",
    "        if len(temp_tokens) == int(len(dataset['tokens']) * size):\n",
    "            break\n",
    "    \n",
    "    for i, tags in enumerate(temp_ner):\n",
    "        temp_ner[i] = [reverse_encoding[tag] for tag in tags]\n",
    "    for i, tags in enumerate(temp_pos):\n",
    "        temp_pos[i] = ['O' for tag in tags]\n",
    "\n",
    "    # print('size', len(temp_tokens))\n",
    "    return {'tokens':temp_tokens, 'pos_tags':temp_pos, 'ner_tags':temp_ner}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "176e920d-f59c-4a74-bc6d-6834b7218264",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train = new_dict(dataset['train'], 'train', 0.1)\n",
    "new_data_train = Dataset.from_dict(new_train)\n",
    "dataset['train'] = concatenate_datasets([dataset['train'], new_data_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c57cd8ff-e433-4777-922b-67f74b7259d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_val = new_dict(dataset['validation'], 'validation', 0.1)\n",
    "new_data_val = Dataset.from_dict(new_val)\n",
    "dataset['validation'] = concatenate_datasets([dataset['validation'], new_data_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "989c8774-467d-45ed-afa0-d08c8caf6228",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user/HS225/os00315/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/user/HS225/os00315/.local/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "eng = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def lemmatize_tokens(dataset):       \n",
    "    dataset['tokens'] = [eng(token)[0].lemma_ for token in dataset['tokens']]\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfb8f616-58dd-4288-870a-f67489a9c44a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a68be356bc347c1ac60c8dd670add51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1179 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71025b601118466487c423119efe14d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/138 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffa0331c992c41e3a3561853b446b7c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/153 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(lambda dataset: lemmatize_tokens(dataset))\n",
    "\n",
    "train_dataset = dataset[\"train\"]\n",
    "val_dataset = dataset[\"validation\"]\n",
    "test_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "686f4cbc-7103-44e4-aa16-4f1d84511a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "881ec152-d344-4ffa-b242-8e885163bf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_label(tags):\n",
    "    global label_encoding\n",
    "    return [label_encoding[tag] for tag in tags]\n",
    "\n",
    "def build_vocab(dataset):\n",
    "    global  word_index\n",
    "    for item in dataset['tokens']:\n",
    "        for word in item:\n",
    "            if word not in word_index:\n",
    "                word_index[word] = len(word_index)\n",
    "    return word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e022df6-1a8a-4c05-aea7-55168e5985de",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = build_vocab(train_dataset)\n",
    "word_index = build_vocab(val_dataset)\n",
    "word_index = build_vocab(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c058edf-45c8-45b5-9e71-135d3ad259f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(word_index, word2vec_model, embedding_dim):\n",
    "    vocab_size = len(word_index) + 1 \n",
    "    embedding_matrix = torch.zeros(vocab_size, embedding_dim)\n",
    "    for word, idx in word_index.items():\n",
    "        try:\n",
    "            embedding_matrix[idx] = torch.tensor(word2vec_model[word], dtype=torch.float32)\n",
    "        except KeyError:\n",
    "            # Initialize unknown words with a zero vector\n",
    "            embedding_matrix[idx] = torch.zeros(embedding_dim)\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f8a9a5-4081-466a-96a5-000dfe20a561",
   "metadata": {},
   "source": [
    "## Define the Bi-LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd8761d9-943a-4434-b355-019f7758f538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bi-LSTM model\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tags, pretrained_embeds):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embeddings = nn.Embedding.from_pretrained(pretrained_embeds, freeze = False)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(2*hidden_dim, tags)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        lstm_out = self.fc(lstm_out)\n",
    "        return torch.log_softmax(lstm_out, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc190a67-4f69-454e-91db-bc46ef43f854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Encode strings and return tensor\n",
    "def encode_sequence(seq, encoder=None):\n",
    "    if type(seq[0]) == int:\n",
    "        encoded_ids = seq\n",
    "    else:\n",
    "        encoded_ids = [word_index.get(word,0) for word in seq]\n",
    "    return torch.tensor(encoded_ids, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56791da2-d617-4172-9708-fc369206fd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "def evaluate(model, token_dataset, word_index):\n",
    "    global label_encoding\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, datapoint in enumerate(token_dataset):\n",
    "            inputs = encode_sequence(datapoint['tokens'], word_index)\n",
    "            targets = encode_sequence(encode_label(datapoint['ner_tags']))\n",
    "            tag_scores = model(inputs.unsqueeze(0))\n",
    "            \n",
    "            predicted_tags = tag_scores.max(-1)[1].squeeze().tolist()\n",
    "            y_pred.extend(predicted_tags)\n",
    "            y_true.extend(targets.tolist())    \n",
    "\n",
    "    # Detailed performance metrics\n",
    "    labels_indices = list(label_encoding.values())\n",
    "    labels_names = list(label_encoding.keys())\n",
    "    print(classification_report(y_true, y_pred, labels=labels_indices, target_names=labels_names))\n",
    "\n",
    "    overall_f1 = f1_score(y_true, y_pred, average= 'macro')\n",
    "    overall_precision = precision_score(y_true, y_pred, average='macro')\n",
    "    overall_recall = recall_score(y_true, y_pred, average='macro')\n",
    "    print(f'Overall F1 Score: {overall_f1}')\n",
    "    print(f'Overall Precision Score: {overall_precision}')\n",
    "    print(f'Overall Recall Score: {overall_recall}')\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[labels_indices])\n",
    "    disp = sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels_names, yticklabels=labels_names)\n",
    "    disp.set_xlabel(\"Predicted Label\")\n",
    "    disp.set_ylabel(\"True Label\")\n",
    "    disp.set_title(\"Confusion Matrix for NER-tags\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d137467-e288-46d6-8b5c-97bab8b4fd97",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7aebb3-97f3-49ca-9c87-05b11083b077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "embedding_matrix = create_embedding_matrix(word_index, word2vec_model, embedding_dim)\n",
    "model = LSTM(embedding_dim, hidden_dim, len(word_index), 4, embedding_matrix)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.9)\n",
    "\n",
    "for epoch in range(6):\n",
    "    for datapoint in train_dataset:\n",
    "        model.zero_grad()\n",
    "        sentence_in = encode_sequence(datapoint['tokens'])\n",
    "        targets = encode_sequence(encode_label(datapoint['ner_tags']))\n",
    "        tag_scores = model(sentence_in.unsqueeze(0))\n",
    "        loss = loss_function(tag_scores.squeeze(0), targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # if i == 199:break\n",
    "    \n",
    "    # Model Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for val_datapoint in val_dataset:\n",
    "            val_inputs = encode_sequence(val_datapoint['tokens'], word_index)\n",
    "            val_targets = encode_sequence(encode_label(val_datapoint['ner_tags']))\n",
    "            val_tag_scores = model(val_inputs.unsqueeze(0))\n",
    "            val_loss = loss_function(val_tag_scores.squeeze(0), val_targets)\n",
    "            \n",
    "    model.train()\n",
    "    \n",
    "# Evaluate on the test set\n",
    "evaluate(model, test_dataset, word_index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
